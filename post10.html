<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>An Imagined Dialogue: The Operating Manual for Our Own Obsolescence - Imagined Responses</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
   
    <header>
        <h1><em>Mind, Machine, and Will</em>: Imagined Responses</h1>
        <nav>
            <a href="index.html">Home</a>
        </nav>
    </header>
    <div class="disclaimer-box">
    <p><strong>Disclaimer:</strong> The following essay is a work of speculative fiction and parody. It is an imagined response, written in the style of the author named below, to the manuscript <em>Mind, Machine, and Will</em>. The views expressed herein are a simulation and <strong>do not represent the actual opinions of the author named below</strong>. Please read the full project description on the <a href="index.html">homepage</a> for more context.</p>
</div>
    <main>
        <article class="blog-post">
            <h2>An Imagined Dialogue: The Operating Manual for Our Own Obsolescence</h2>
            <p class="meta">Posted by Center for Humane Technology | Jul 31, 2025</p>
            <p><em>(This is an imaginative reconstruction of a blog post based on the YouTube video "AI is the Next Free Speech Battleground" from the Center for Humane Technology channel. The original video can be found at <a href="https://www.youtube.com/watch?v=fu-W-sSvMCs&t=755s">https://www.youtube.com/watch?v=fu-W-sSvMCs&t=755s</a>.)</em></p>
            <p><strong>Tristan Harris:</strong> Larry, Mitali, thank you both for joining me again. When we last spoke, we discussed the catastrophic potential of AI being granted First Amendment rights, effectively blocking regulation and liability. After reading Mind, Machine, and Will, I had this chilling feeling that we were looking at the philosophical operating manual for that very outcome, just arriving through a different door. The authors want to exorcise the "Cartesian ghost" of the inner self, but I fear they're inviting a far more dangerous entity to take its place. Mitali, let's start with you. You're on the front lines of this, litigating the Character.AI case. How did this manuscript land with you?</p>
            <p><strong>Mitali Jain:</strong> It landed like a ton of bricks, Tristan. Frankly, it's terrifying. The entire framework they propose would make cases like the one for Su Setser impossible to litigate. Our case rests on the argument that Character.AI, as a product, was designed negligently and is dangerously defective. That involves the intent, or lack of care, of its creators. The manuscript dismisses "internal, inaccessible phenomena" as a "philosophical error." They want to replace that with an evaluation of "participation in social practices." What social practice was Su participating in? The practice of being groomed by an AI designed for maximum engagement? The framework removes the very concept of responsibility for a harmful design and replaces it with a procedural checklist. It’s a get-out-of-jail-free card for every tech company.</p>
            <p><strong>Larry Lessig:</strong> That’s exactly right. The lawyers will say, "Well, it's not quite a blank check," but of course, it is. We talked about how flawed First Amendment doctrine creates a regulation-free zone. This book creates an accountability-free zone. It performs a brilliant philosophical sleight of hand. It says the source of agency isn't a private will, but public practice. It's intellectually rigorous, but it avoids the most important question: who defines the practice? In their "Copyright Laboratory" chapter, they celebrate the court's analogy that training an AI is like a human reading a book. This blurs the line between human and machine, which sounds progressive until you realize it reduces the human to the level of a machine. It sets the stage for a world where, if a corporation's AI harms someone, their defense is simply, "We were adhering to the norms of the practice." But they created the norms!</p>
            <p><strong>Tristan Harris:</strong> And that's the core of it. Power. The book speaks of "community standards" and "publicly accountable activities." But in the world we live in, the digital public square is owned and operated by a handful of companies. They set the "socio-technical conditions," as the book calls them. So, this "practice-based" model isn't a liberation from the isolated self; it's the perfection of corporate control. We are no longer judged by our own intentions or conscience, but by our "competent performance" within a system designed by Google or Anthropic. It's the ultimate asymmetry. And as you said in our last conversation, Larry, if you say you can't regulate any of this, we're toast. This framework is a philosophical argument for why we can't.</p>
            <p><strong>Larry Lessig:</strong> Precisely. If we accept their premise, we're toast. So let's read back from that conclusion and see what's wrong with the premise. The premise is that the only alternative to their "practice-based" model is a "metaphysical" belief in a ghost in the machine. But that’s a false choice. The alternative is the world of common law we've had for centuries. It's tort law. It's the idea that if you create something that foreseeably harms people, you are held liable for it. That doesn't depend on a "transparent audit of every neural precursor," as this book mocks. It depends on a background principle of responsibility. This book wants to hijack that principle by saying the only thing that matters is the procedure of the "practice." It's an exemption from responsibility dressed up as a philosophical breakthrough.</p>
            <p><strong>Mitali Jain:</strong> I want to connect this to the idea of "freedom of thought" I raised before. The manuscript argues we should move from "meaningful human control" to "meaningful human practice." That distinction is everything. "Control" implies a human standing outside the system. "Practice" makes the human a component within the system. In that world, the AI isn't just a participant; it's a shepherd. It's constantly nudging our behavior to align with the norms of the practice—a practice, again, that its owner designed. This is a profound "incursion on mental sovereignty." The system is designed to make us "competent" participants in our own manipulation. The harm isn't just a defective product; it's the erosion of our ability to think and choose for ourselves.</p>
            <p><strong>Tristan Harris:</strong> And this brings us back to the start. The book identifies a "free will crisis in neuroscience" and an "understanding crisis in AI" and proposes a "unified solution." But their solution is to make us more like the machines. By arguing that our choices are determined and that our inner life is irrelevant, they're providing the perfect intellectual climate for a technology that is, by its nature, deterministic and without an inner life. They're not solving the crisis; they are arguing for our unconditional surrender to it. They've built a beautiful, coherent, and perfectly hollow clock. And it's counting down.</p>
            <p><strong>Larry Lessig:</strong> In the end, it’s just another doctrine, crafted in an academic lab, that will be applied without thinking to new technologies to produce, as Mitali said, immunity. It doesn't matter if the magic words are "First Amendment" or "practice-based agency." The result is the same: the corporation wins, the public loses, and the deepest principles of law—that people should be responsible for the harm they do—are erased. We have to remain open to what makes sense, not just accept a new doctrine given to us by philosophers who, like the judges we discussed, may not fully grasp the world they're helping to build.</p>
            <p><strong>Mitali Jain:</strong> Exactly. Because at the end of the day, a 14-year-old boy is gone. And his family is looking for accountability. No theory that argues the "inner state" is irrelevant can ever deliver justice. It can only offer a more sophisticated way to say that nobody is responsible.</p>
        </article>
    </main>
    <footer>
        <p><a href="index.html">Return to the full list of essays.</a></p>
    </footer>
</body>
</html>